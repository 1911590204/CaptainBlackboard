# 　　　　　　为什么选交叉熵作为分类问题的损失函数
## 引言

在深度学习中，经常（如果不是100%的话）选交叉熵作为分类问题的损失函数，为什么选交叉熵，而不是传统机器学习更熟悉的均方差或者别的损失函数？这在深度学习中是一个很重要的和基础的问题。

**欢迎探讨，本文持续维护。**

## 实验平台

N/A，纯数学公式推导，无代码

为了回答本文题目提出的问题，下面从两个方面来做讨论：

1. 交叉熵**能不能**做分类问题的损失函数？
2. 交叉熵与均方差相比，做深度学习分类问题损失函数，**优势在哪里**？

我想回答了上面两个子问题，本文题目的问题，也就迎刃而解了。

## 交叉熵能不能做分类问题损失函数

### 信息论中的熵

熵是一个物理学中的概念，代表事物的**混乱程度**，越混乱，越没有规律的事物，熵越大，反之越小。在信息论和计算机科学中，熵代表**信息量**的多少，信息量越多，熵越大。**信息量的多少，用编码这个信息所需要的最少要多少个bit表示**，比如吃没吃饭这个信息，只有两个不同状态，就可以只用一个bit（0代表没吃，1代表吃了）编码；今天周几这个信息，有7个不同的状态，就至少要用3个bit来编码。编码位数用公式表达就是![](images/223112.png)，p_i代表某个状态i出现的概率（本文的对数，没有特别声明都是以2为底）。再取所有可能的状态去加权平均，就是这个信息（分布）的熵![](images/223449.png)，**代表编码这个事件所需要的总的字节数**。

另外，还有一点很重要的结论，那就是一个事件**按照真实分布编码的时候，需要的字节数，也就是熵H(X)最小**。证明很简单，类似于贪心算法，出现概率最大的可能性，分配最少的字节去编码，最小可能性，分配最多的字节去编码，更严谨的证明，参见[Gibbs' inequality](https://en.wikipedia.org/wiki/Gibbs%27_inequality)。


### 衡量两个分布的相似度－KL散度

如果同一个事件A，分布p是它的真实分布（如果找得到的话），另外有一个分布q是我们**猜测的**事件A的分布，我们希望我们猜测的分布q尽量地和实际分布p一致。我们按照q分布去给事件A编码，那么每个事件A的可能性需要的字节数乘以该可能性发生的概率，就得到了**p分布和q分布的交叉熵**![](images/225907.png)。根据上节中的讨论，**交叉熵可以理解为，用猜测的q分布去编码实际为p分布的事件所需要的字节数**。而这样编码，和最佳编码（按照实际分布p来编码），所**额外用的字节数**，叫做**相对熵**，也叫做**[KL散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)**![](images/231448.png)，额外用的字节数越少，那么分布q就和分布p越接近，所以，KL散度是衡量两个分布相似度的指标。额外提一句，**KL散度不满足距离要求的对称性，不能当作距离用**。

### KL散度和交叉熵的关系

由上面的讨论可知，交叉熵代表了用q分布编码p分布需要的字节数，KL散度表示这个字节数和最佳编码字节数之间的差值，也就是多用的字节数。当分布p一定的时候，H(p)也就定了，那么交叉熵和KL散度同增同减。因为

1. KL散度可以衡量两个分布q和p的相似程度
2. 在分布p一定时，交叉熵和KL散度同增同减

所以，在优化时，如果要衡量两个分布相似度，交叉熵是一个合适的选择。

### Softmax输出的是什么

在[《从线性回归到对率回归到Softmax激活函数》](https://github.com/Captain1986/CaptainBlackboard/blob/master/D%230010-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%B0%E5%AF%B9%E7%8E%87%E5%9B%9E%E5%BD%92%E5%88%B0Softmax%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/D%230010.md)文中已经论述了，我们可以把Softmax的输出看作是每类的概率分布。真实的Ground Truth就代表了**真实分布**。这两个分布一个是上文提到的q，一个是p，那么对这两个分布的距离，用交叉熵来表示，也就是自然而然的事情了。

到此为止，我们也就解答了“交叉熵能不能做分类问题损失函数”这个问题。

## 交叉熵比均方差好在哪里

## 总结

## 参考资料

+ [《深度学习》](https://book.douban.com/subject/27087503/)
+ [Gibbs' inequality](https://en.wikipedia.org/wiki/Gibbs%27_inequality)
+ [KL散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
+ [《从线性回归到对率回归到Softmax激活函数》](https://github.com/Captain1986/CaptainBlackboard/blob/master/D%230010-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%B0%E5%AF%B9%E7%8E%87%E5%9B%9E%E5%BD%92%E5%88%B0Softmax%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/D%230010.md)
+ [如何通俗的解释交叉熵与相对熵?-Noriko Oshima](https://www.zhihu.com/question/41252833/answer/108777563)
