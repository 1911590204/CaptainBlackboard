# 　　　　　　支持向量机
## 引言
在机器学习中，[支持向量机](https://en.wikipedia.org/wiki/Support-vector_machine)是一种有效并且常用的二分类模型，它的基本模型是定义在特征空间中的间隔最大化的线性分类器（加上核技巧后也可变成非线性分类器）。它的数学理论基础扎实，且适用于小样本的学习，在现在深度学习流行的今天，也是一种值得学习的基础模型。

**欢迎探讨，本文持续维护。**



## 实验平台

N/A，纯数学公式推导，无代码



## 线性支持向量机

### 从二分类模型到线性支持向量机基本型
给定一组训练数据{(x1,y1), (x2,y2), ..., (xN,yN)}，其中x是特征向量，y是标签（取值-1，+1）。二分类问题就是要找到一个假设函数h(x)，使得对所有的训练数据有h(xi) = yi，数学描述等价于h(xi) \* yi = 1。如果我们假设要找的这个假设函数为线性函数，即假设h(x) = sign(Wx + b)这种形式，那么就是线性二分类问题，Wx + b = 0为**划分超平面**，h(x) = Wx + b为**分类决策函数**，超平面将整个特征空间一分为二，符合h(x) > 0的决策为+1类，否则决策为-1类。数学描述就是要找到合适的W和b参数，使对所有h(xi) \* yi = sign(Wxi + b) \* yi = 1，这等价于对所有的xi都有(Wx + b) * yi > 0。

支持向量机的核心思想就是，选择和距离正样本集合和负样本集合**间隔最大**的超平面为划分超平面(而空间点和平面的距离公式为下图中式3所示)，所以支持向量机的原始形式可以见下图式4和式5，翻译成人话就是，在分类所有训练样本正确的前提下，寻找W和b，使其构成的超平面Wx + b = 0可以最大距离划分正样本集和负样本集，所谓的最大距离，就是超平面和任意一个样本点的距离都尽可能地大。

![](images/160121.jpg)

但是这里原始形式很难处理，为了能在能在现实中好优化，需要继续把它做简化，简化成一个经典的凸二次规划(QP)问题。

在**式4中，里面的求最小，是相对于所有的样本点求最小，和W,b的取值无关**，所以可以将式4简化为式6，然后可以注意到，同时用相同的系数缩放W和b，不改变式6和式5的解，我们就可以进一步令min|Wxi+b| = 1（反正可以任意缩放咯），这里的集合意义是让样本点中离超平面最近的距离归一化一下，然后式6就可以转化到式7(等价可以到式8)，同样式5可以转化为式9。最后联立式8和式9就是线性支持向量机的基本型了。



### 线性支持向量机与硬间隔最大化

### 线性支持向量机与软间隔最大化



## 非线性支持向量机

### 核技巧基本原理

### 常用核函数



## 支持向量机与深度学习讨论



## 总结



## 参考资料

+ [《统计学习方法》](https://book.douban.com/subject/10590856/)
+ [《PRML》](https://www.douban.com/group/471521/)
+ [《The Elements of Statistical Learning》](https://book.douban.com/subject/3294335/)
+ [南京大学-张皓-从零推导支持向量机](https://github.com/HaoMood/File/blob/master/%E4%BB%8E%E9%9B%B6%E6%8E%A8%E5%AF%BC%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA(SVM).pdf)
