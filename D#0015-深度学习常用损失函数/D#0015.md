# 　　　　　　深度学习常用损失函数
## 引言

深度学习的任务本质上来说是拟合一个函数Fcnn，将输入的图片映射到对应的标签，这个映射的好坏需要用损失函数来表达，学习训练的过程也是由损失函数来指导。本文总结了一些在深度学习，更具体地说，实在卷积神经网络中常见的损失函数及其特性和使用场合。

**请注意：本文为了讨论方便，不严格区分目标函数，损失函数。**

**欢迎探讨，本文持续维护。**

## 实验平台

N/A

卷积神经网络常见的任务有两种，一种是分类（输出离散值，例如预测输入图片的类别），一种是回归（输出连续值，例如预测回归框的相对偏移量）。这两种任务都有各自适合的损失函数，**有时候是多种损失函数的组合**。这里为了方便，就以任务的不同来分别讨论各自的损失函数。

## 分类问题常用损失函数

### 交叉熵

一般来说，分类网络最后一层是用Softmax来输出预测各类别的概率值，KL散度是评价两个概率值的常见手段，交叉熵损失就是脱胎于KL散度，用来评价Softmax输出的概率分布与训练样本真值概率之间的差异性。而且，在优化的角度来讲，交叉熵损失也适合Softmax。关于交叉熵损失更详细的理解和说明，可以参见我前面写的[《为什么选交叉熵作为分类问题的损失函数》](https://github.com/Captain1986/CaptainBlackboard/blob/master/D%230012-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E4%BA%A4%E5%8F%89%E7%86%B5%E4%BD%9C%E4%B8%BA%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/D%230012.md)。

### 合页损失函数

合页损失函数L = max(0, 1-x)，其函数曲线如下图蓝色线所示，类似于一个合页而得名。合页损失函数起源于SVM，它的特点是对错分类的点，考虑到它距离分类平面的远近，距离越大则损失越大；但是对于能正确分类的样本，如果这些样本离分类平面的距离不够大的话，也会有一点损失。这样就强迫了在优化的时候，优化过程去寻找最大间隔划分平面。关于它的讨论，可以参见我前面写的[《支持向量机》](https://github.com/Captain1986/CaptainBlackboard/blob/master/D%230009-SVM/D%230009.md)一文。

![](images/154349.png)

### 坡道损失函数

合页损失函数对于错误越大的样本，施加的惩罚越严重。可是样本毕竟不是那么完美的，如果里面有一些标签错误的，或者本身就是离群点（outlier），那么就会误导合页损失，从而影响分类超平面的学习。

为了缓解这个问题，坡道损失函数（其函数图像如上图红色曲线），限制了损失的最大值，在一定程度上缓解了前面讨论的错误标签和离群点带来的问题。

### 中心损失函数

上面介绍的损失函数，仅仅只考虑了学到让不同类与类之间间隔增大的特征；而**良好的特征，不仅仅要考虑不到不同类之间间隔要大，应该也要保证同类之间的间隔要小，这样的特征更有辨别力**。中心损失函数，就可以指导网络学习到的特征，在同类的时候尽量聚拢在一起。bianbieli

![](images/160647.png)

上式为中心损失函数的表达式，其中c_yi为第yi类训练样本深度特征的均值点。网络学习到特征离离中心点越远，则惩罚越大，这就促使网络去学让每个同类别样本能聚拢在一块的特征。

在实际使用中，由于中心点损失函数值考虑类内差异性，而交叉熵损失函数只考虑类间差异性，一般会把中心损失函数和交叉熵损失函数配合起来用各取所长。这样网络最终的目标函数可以表示为：

![](images/161349.png)

其中，lamda代表了两种损失的相对比例，用来调节学习到的特征类间分离性和类内聚拢性之间的相对强度，lamda越大，类内聚拢越强烈。

![](images/161758.png)

如上图所示，为做0~9这十个手写数字分类任务时，用L_final目标函数优化网络，取不同lamda值所学习到的特征投影到二维平面的图像。可以明显地观察到，当中心损失函数比重增大时，网络学习到的特征更集中，类间差异更小，学习到的特征更好。

在分类性能方面，**中心损失函数搭配传统交叉熵损失函数作为目标函数学习到的特征要优于只使用传统交叉熵损失函数作为目标函数所学习到的特征**，特别是在人脸识别问题上可能有较大的性能提升。

### Focal Loss

## 回归问题常用损失函数

![](images/164439.png)

### L1 Loss

L1损失如上图最左边所示，它常作为正则项来减轻模型过拟合或者使网络稀疏化（可参看[《拉格朗日乘子法》](https://github.com/Captain1986/CaptainBlackboard/blob/master/D%230008-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95/D%230008.md)一文介绍），做特征选择，模型压缩用。

L1损失直接对网络输出的预测值和真值的差值取绝对值，计算简单，原理直观。

### L2 Loss

L2损失如上图中间所示，它经常作为正则项来减轻模型的过拟合。L2损失求导简单。

### Tukey's biweight

Tukey's biweight损失函数是一种克服噪声和离群点的鲁棒损失函数

### Smooth-L1 Loss

Smooth L1损失首次是在Fast RCNN算法里面作为Bounding Box Regression的损失函数提出，它是L1损失的改良版本，和普通L1损失和L2损失相比，有如下三点优势:

1. **第一，它在训练的后期**，当预测值与真值相差很小的时候，Smooth L1的导数比L1的导数更小，可以**使训练达到更高的精度**。

2. **第二，在训练的前期**，当预测值和真值相差很大的时候，Smooth L1的导数相比于L2的导数不会太大，**避免了训练不稳定**的情况出现。

3. **第三，对于数据噪音和离群点**，Smooth L1的损失比L2的损失小，不容易误导网络训练。换句话说就是，对噪音和离群点更鲁棒。

基于以上优点，在Fast RCNN之后，很多流行的目标检测算法，比如SSD，也都采用了Smooth L1这种损失。

## 总结

## 参考资料

+ [《深度学习》](https://book.douban.com/subject/27087503/)
+ [CNN_book](http://210.28.132.67/weixs/book/CNN_book.pdf)
